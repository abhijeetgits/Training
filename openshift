Open shift is a hybrid cloud

OPen shift  kubernettesby RHEL is more reliable and they provide live time support
there are kubernettes and AWS ekd

1)docker hub is a place where we can check the images of the container installed
by someone else


> docker search ubuntu

dont download images which are not official,as they might contain ransomware etcs malwares
 
in ubuntu machine apt-get update works but un centos  yum repolist works

> docker run -it ubuntu

instead of installing virtual machine ,we can install containers of ubuntu very quickly

> docker ps to check how many containers are running

2) Here we are using operating system as microservices
3) Kubernettes is used for managing containers
kubernettes was created by google and it donated it cncf
pokemon go uses kubernettes, they had requirements to scale for 1 million users per seconds
hbo is also using kubernettes
redhat has added a lyer of openshift on top of kubernettes, 

4)abhidocks23
5)dockerhub is actually using github as background
we can install container lets say ubuntu and we can install java using 
apt-get update
the main concept of openshift containers is not to install operating system

there are 2 types of linux used RPM And DEBIAN

UNIX was asked by one developer to shared the kernel details butthey refused
by gtelling that they are closed source
hr created luinux kernel, then another companies like RHEL,UBUNTU,CENTOS came and used
these kernels to craete 2 diffrent  falvours RPM based and DEBIAN based

for openshift we have 1 free open source upstream called as okd and its enterprise
version is openshift

docker ee is enterprise edition
docker community edition : post questions and if similar problems arethere
they will push it to company
RH has 3 types of s/ws 

1) FEDORA -upstream for red hat enterprise linux
2)CENTOS
3)REL 

1ST AND 2ND are open source
centos simply downloaded iso image file and simply removed all branding of RH and simply
sold it as its own,
REL is enterprise verion of RH

OKD : openshift container daemon 
requirements 16gb ram and quad core cpu

openshift runs on kubernettes and kubernettes run on pods
we dont use container 

openshift is a cloud and docker is software
docker is a part of openshift

docker cretaes container

limux kernel treats contaimner as a process

pod is a sandbox with multiple containers

pod may contain container all with java, its an isolated environment where each 
project runs

if we have to launch conatiner in docker we have to be root

sudo is different from root
root allows you to delete files also in ubuntu,but sudo (super doer) allows only to
modify the / files

docker took lot of time to resolve the issue of rootless conatiners
strong linux knowledge is needed for openshift
>kubectl get projects

redhat create another layer on top of kubernettes

OC
!
kUBERNETTES

>OC get projects

Open shift is kubernettes solution
open shift is orcahestration tool
>OC status

open shift has web console also where we need not type in commands
2 registries are there
1)docker registry
2) redhat registry

all the images are stored in these regsitries

once we install openshift we will be getting dockers and kubernettes

1) for installing centos
download the iso image copy
go to intallation manager
search for virtaul machin manager
run centos

for installing docker for centos
>yum repolist

1  yum repolist
    2  REPO -repository
    3  sudo yum install -y yum-utils   device-mapper-persistent-data   lvm2
    4  yum-config-manager     --add-repo     https://download.docker.com/linux/centos/docker-ce.repo
    5  yum-config-manager --enable docker-ce-nightly
    6  his
    7  history
    
    [root@localhost ~]# cat /etc/redhat-release
CentOS Linux release 7.6.1810 (Core) 
[root@localhost ~]# systemctl start docker
(to start the docker)

1  yum repolist
    2  REPO -repository
    3  sudo yum install -y yum-utils   device-mapper-persistent-data   lvm2
    4  yum-config-manager     --add-repo     https://download.docker.com/linux/centos/docker-ce.repo
    5  yum-config-manager --enable docker-ce-nightly
    6  his
    7  history
    8  yum install docker-ce docker-ce-cli containerd.io
    9  cat /etc/redhat-release
   10  systemctl start docker
   11  docker search ubuntu
   12  docker run -it ubuntu
   13  docker search ubuntu
   14  docker search java
   15  docker ps
   16  docker container ls
   17  docker ps -a  (to see the name of container)
   18  history

hypervisor is a tool which is used to create virtual machine

   
virtual machine is a operating system on another machine
eaelir we use to hvae hardware and we used to create partitions and manually install
s/ws on the hardware partitions

but with virtualization we need not do that
in containers when we are downloading ubuntu,it will download only limited features of
ubuntu, thats why when we download ubuntu on server it will require 834 mb but if we download
ubunut in docker it needs only 84 mb,because it has removed bloatwares ,like bloatwares
are something we dont use but they bloat our system

NOw its the responsibility of host kernel to provide resources to containers

1  yum repolist
    2  REPO -repository
    3  sudo yum install -y yum-utils   device-mapper-persistent-data   lvm2
    4  yum-config-manager     --add-repo     https://download.docker.com/linux/centos/docker-ce.repo
    5  yum-config-manager --enable docker-ce-nightly
    6  his
    7  history
    8  yum install docker-ce docker-ce-cli containerd.io
    9  cat /etc/redhat-release
   10  systemctl start docker
   11  docker search ubuntu
   12  docker run -it ubuntu
   13  docker search ubuntu
   14  docker search java
   15  docker ps
   16  docker container ls
   17  docker ps -a
   18  history
   19  docker run -it -d ubuntu
   20  docker attach c177eb4ad7c6e2fc333fca5aa167bcf92a965315a5e6f453996beaf6e4d8c107
   21  history
   https://docs.docker.com/install/linux/docker-ce/centos/
   
   
  ceph storage-persistent volume?
  cluster storage
  other storage
  
  >oc get events
  pod is like a sandbox where we can create,modify and autoscale 
  
  persistent volume can be used for storing database
  if our containers gows down ,we can spin off new container and can attach 
  new conatiner with old persistent volume
  
  . oc new 
  
  > oc logs -f -f bc/ruby-ex
  
  podman has its own registry QUAY
  > oc get pods 
  . oc describe pods
  
  $ sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine
                  
           
 INstallation prerequisites
 Centos
 OKD
 16gb ram
 dual core cpu
 min 40 gb free HDD space
 10 MBPS INTERNET CONNECTION
 
 oRIGIN KUBERNETTES DEPLOYMENT
 upstream is terminlogy used for open source products which are created by community
 1  ping www.google.com
    2  yum repolist
    3  yum remove docker                   docker-client                   docker-client-latest                   docker-common                   docker-latest                   docker-latest-logrotate                   docker-logrotate                   docker-selinux                   docker-engine-selinux                   docker-engine
    4  ifcongig
    5  ifconfig
    6  yum repolist
    7  ping www.google.com
    8  yum repolist
    9  docker search ubuntu
   10  docker ps
   11  yum remove docker-ce-cli.x86_64
   12  yum remove containerd.io.x86_64
   13  his
   14  history
   
   in redhat machine
   hostname
    8  hostnamectl set-hostname abhi.openshifts.com
    9  hostname
    
    1) Installing one node cluster
    
    its having master node and infra node on same machine
    >create key suing sshkeygen
    ideal thing ius to have master on one node and infra on another node
    master communicates with other nodes using ssh
    
    [root@abhi ~]# ssh-copy-id abhi.openshifts.com
    
    this command will send the ssh key to host name mentioned
    out of compute and master ,comput requires more cpu
    
    root@abhi ~]# ssh-copy-id abhi.openshifts.com
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"
The authenticity of host 'abhi.openshifts.com (fe80::3f5d:1ea:503c:a46d%eno1)' can't be established.
ECDSA key fingerprint is SHA256:RK/PxBuSG+MAhRhRY7m7m6l8KDK1kdefkOYsi5vh8Og.
ECDSA key fingerprint is MD5:78:2e:64:33:ec:2d:2e:32:f5:62:14:e7:f7:20:30:07.
Are you sure you want to continue connecting (yes/no)? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
root@abhi.openshifts.com's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'abhi.openshifts.com'"
and check to make sure that only the key(s) you wanted were added.

[root@abhi ~]# ssh abhi.openshifts.com
Last login: Thu Apr  4 11:19:52 2019 from abhi.openshifts.com
[root@abhi ~]# 

oncentos

18  ifconfig
   19  ssh 192.168.122.73
   20  wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
   21  rpm -ivh epel-release-latest-7.noarch.rpm
   22  yum update
   23  ssh key-gen
   24  ssh keygen
   25  ssh-keygen
   26  hostname
   27  hostnamectl set-hostname abhi2.openshifts.com
   28  ssh-copy-id abhi2.openshifts.com
   29  yum install  -y wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct
   30  reboot
   31  yum -y install                     https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
   32  history

do yum update now

1) M -N(compute)
  (etcd)
   |
   n
   (compute)

etcd node contains information about other nodes

in order to make your cluster high available it is better to keep more then master node ,we always keep it in odd number ,

elts say if 1 master goes down ,another is there and if another goes down 3rd one is there

---------HA----------------
| |MN|-----|MN|----|MN|
| |etcd|----|etcd|---|etcd| |
|  1-100 compute node       |





hostname
    8  hostnamectl set-hostname abhi.openshifts.com
    9  hostname
192.168.1.39)

https://www.tecmint.com/how-to-enable-epel-repository-for-rhel-centos-6-5/

extra package for enterprise linux

# wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
# rpm -ivh epel-release-latest-7.noarch.rpm
The OKD installer requires a user that has access to all hosts. If you want to run the installer as a non-root user, first configure passwordless sudo rights each host:

1>    Generate an SSH key on the host you run the installation playbook on:

    # ssh-keygen

    Do not use a password.

	

 2>   Distribute the key to the other cluster hosts. You can use a bash loop:

    # for host in master.example.com \     * 
        node1.example.com \                            *
        node2.example.com; \                           *
        do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; \
        done

    	- * Provide the host name for each cluster host.

    Confirm that you can access each host that is listed in the loop through SSH.



Installing base packages
	

If your hosts use RHEL 7.5 and you want to accept OKD’s default docker configuration (using OverlayFS storage and all default logging options), do not manually install these packages. These packages are installed when you run the prerequisites.yml playbook during installation.

If your hosts use RHEL 7.4 or if they use RHEL 7.5 and you want to customize the docker configuration, install these packages.
For RHEL 7 systems:

    Install the following base packages:

    # yum install  -y wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct

    Update the system to the latest packages:

    # yum update
    # reboot

    Install packages that are required for your installation method:

        If you plan to use the containerized installer, install the following package:

        # yum install atomic


        If you plan to use the RPM-based installer:

            Install Ansible. To use EPEL as a package source for Ansible:

                Install the EPEL repository:

                # yum -y install \
                    https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

                Disable the EPEL repository globally so that it is not accidentally used during later steps of the installation:

                # sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo

                Install the packages for Ansible:

                # yum -y --enablerepo=epel install ansible pyOpenSSL

            Clone the openshift/openshift-ansible repository from GitHub, which provides the required playbooks and configuration files:

            # cd ~
            # git clone https://github.com/openshift/openshift-ansible
            # cd openshift-ansible
            # git checkout release-3.11



Installing Docker

At this point, install Docker on all master and node hosts. This allows you to configure your Docker storage options before you install OKD.
	

The cluster installation process automatically modifies the /etc/sysconfig/docker file.
For RHEL 7 systems:

    Install Docker 1.13:

    # yum install docker-1.13.1

    Verify that version 1.13 was installed:

    # rpm -V docker-1.13.1
    # docker version

Start or restart Docker.

    If Docker has never run on the host, enable and start the service, then verify that it is running:

    # systemctl enable docker   (its for enabling docker,in order to keep it persistently running
    # systemctl start docker
    # systemctl is-active docker
    
    in SG we have an openshift service deployed on another machine, if decide to move to eopnshift
    we will use its clinet side fearures
    
    if we wish to customize authentication with open shift we can request for RFE
     master keeps all metadata information
     
     # This is the default ansible 'hosts' file.
#
# It should live in /etc/ansible/hosts
#
#   - Comments begin with the '#' character
#   - Blank lines are ignored
#   - Groups of hosts are delimited by [header] elements
#   - You can enter hostnames or ip addresses
#   - A hostname/ip can be a member of multiple groups

# Ex 1: Ungrouped hosts, specify before any group headers.

## green.example.com
## blue.example.com
## 192.168.100.1
## 192.168.100.10

# Ex 2: A collection of hosts belonging to the 'webservers' group

## [webservers]
## alpha.example.org
## beta.example.org
## 192.168.1.100

# If you have multiple hosts following a pattern you can specify
# them like this:

## www[001:006].example.com

# Ex 3: A collection of database servers in the 'dbservers' group

## [dbservers]
##
## db01.intranet.mydomain.net
## db02.intranet.mydomain.net
## 10.25.1.56
## 10.25.1.57

# Here's another example of host ranges, this time there are no
# leading 0s:

## db-[99:101]-node.example.com
#Create an OSEv3 group that contains the masters, nodes, and etcd groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_become must be set to true
#ansible_become=true

openshift_deployment_type=origin
openshift_disable_check=memory_availability,disk_availability,docker_image_availability,package_availability

# uncomment the following to enable htpasswd authentication; defaults to AllowAl#lPasswordIdentityProvider
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# host group for masters
[masters]
abhi2.openshifts.com
# host group for masters
[masters]
abhi2.openshifts.com

# host group for etcd
[etcd]
abhi2.openshifts.com

# host group for nodes, includes region info
[nodes]
abhi2.openshifts.com openshift_node_group_name='node-config-all-in-one'
#node1.example.com openshift_node_group_name='node-config-compute'
#node2.example.com openshift_node_group_name='node-config-compute'
#infra-node1.example.com openshift_node_group_name='node-config-infra'
#infra-node2.example.com openshift_node_group_name='node-config-infra

admin commands in 
>oc adm policy add-cluster-role-to-user jack cluster:admin

command to run ansible playboo
[root@abhi2 ansible]# ansible-playbook -i /etc/ansible/hosts playbooks/deploy_cluster.yml 

the host file is in /etc/ansible/hosts





 







